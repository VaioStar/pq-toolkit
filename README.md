# WIMU 24Z

# Features

* CI/CD
* Data analysis tab
* Sample playback
* Downloadable test result reports
* Automatic mkdocs deployment

# Technology Stack

### Python interface

* Python
* MkDocs

### Frontend experiment application

* Typescript
* npm
* React.js
* Next.js

### Backend

* Flask (back-end) (possible continuation in NextJS)

### Database

* PostgreSQL

### CI/CD

* GitLab (?)
* Jenkins (?)
* ...

### Additions

* Prettier
* Jest
* flake8
* venv
* make
* mkdocs

# Testing

* Unit tests

# Roadmap

* ## Week 1: 21/10/2024 - 27/10/2024

  * **Project Introduction:** literature review, analysis of the existing code and project architecture

* ## Week 2: 28/10/2024 - 3/11/2024
  
  * Preparation of the project structure for CI/CD implementation
  
* ## Week 3: 4/11/2024 - 10/11/2024

  * CI/CD implementation in the project

* ## Week 4: 11/11/2024 - 17/11/2024

  * Initialization of the project extension with a test results analysis tool (tab)

* ## Week 5: 18/11/2024 - 24/11/2024

  * Implementation of the analysis tool

* ## Week 6: 25/11/2024 - 01/12/2024

  * Introduction of functionality for sample playback in the tool

* ## Week 7: 02/12/2024 - 08/12/2024

  * Preparation of the project for the implementation of automated mkdocs

* ## Week 8: 09/12/2024 - 15/12/2024

  * Implementation of mkdocs

* ## Week 9: 16/12/2024 - 01/01/2025

  * Analysis of data access layer requirements for creating a test reporting mechanism/tool
  * Christmas break

* ## Week 10: 02/01/2025 - 06/01/2025

  * Implementation of the data access mechanism for test data from the database

* ## Week 11: 7/01/2025 - 12/01/2025

  * Implementation of the report generation tool

* ## Week 12: 13/01/2025 - 23/01/2025

  * Project review and optimization
  * Finalization of the project's technical documentation

# References

[1] Method for the subjective assessment of intermediate quality level of coding systems. Recommendation ITU-R BS.1534-1, 2003. [[pdf]](https://www.itu.int/dms_pubrec/itu-r/rec/bs/R-REC-BS.1534-1-200301-S!!PDF-E.pdf)

[2] B. De Man and J. Reiss, ‘APE: Audio Perceptual Evaluation toolbox for MATLAB’, 04 2014.
[[pdf]](https://www.researchgate.net/publication/273574027_APE_Audio_Perceptual_Evaluation_toolbox_for_MATLAB)

[3] N. Jillings, D. Moffat, B. De Man, and J. D. Reiss, “Web Audio Evaluation Tool: A browser-based listening test environment,” Jul. 2015. [[pdf]](https://www.researchgate.net/publication/282328219_Web_Audio_Evaluation_Tool_A_Browser-Based_Listening_Test_Environment) [[GitHub]](https://github.com/BrechtDeMan/WebAudioEvaluationTool)

[4] A. Vinay and A. Lerch, “Evaluating generative audio systems and their metrics”, 2022. [[pdf]](https://arxiv.org/pdf/2209.00130.pdf)
